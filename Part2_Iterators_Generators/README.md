Key Lessons
===========
## Sequences
- Custom sequences REQUIRE a `__getitem__` method and *should* implement a `__len__` method
- [`@staticmethod`](https://stackoverflow.com/questions/735975/static-methods-in-python)
- `@lru_cache` - memoize the vals from a fn up to the maximum size (defaults to 128)
- `__repr__` - `return f'MyClass(name={self.name})`
- `__add__(self, other)` - self + other -> new memory address
- `__iadd__(self, other)` - self += other -> same memory address
- Above holds for `__mul__` and `__imul__`
- `sorted(iterable, key=lambda x: ..., reverse=False)` - in-place sorting

## Iterables and Iterators
- Iterators require `__next__` and `__iter__` according to the iterator protocol
- Iterables are objects whose `__iter__` method returns an iterator
- Iterators are objects whose `__iter__` method returns itself
    - Iterators are themselves iterables
- `namedtuple('name', 'field1 field2 field3 ...')`
- Lazy iterables (ex. `class Circle` that doesn't calculate its area more than once)
    - `@property` and `@attr.setter` decorators ([link](https://www.geeksforgeeks.org/python-property-decorator-property/))

## Generators
- Functions that `yield` instead of `return`. This makes them lazy iterators
- `yield from` delegates yielding to another iterator
- Intro to [`* unpacking`](https://www.w3schools.com/python/python_tuples_unpack.asp)

## Iteration Tools
`from itertools import _____`

### Slicing
- `islice(start, stop, step)` - slice iterators lazily - only evaluates as much of the iterator as needed

### Selecting and filtering
- `filter(key, iterator)` - get subset of an iterator. The `key` is often a lambda fn
- `filterfalse(key, iterator)` - keep false vals (opposite of `filter`)
- `dropwhile(key, cutoff)` - drops values from an iterator based on the `key` until the `cutoff` condition is satisfied
- `takewhile(key, cutoff)` - gives values from an iterator based on the `key` until the `cutoff` condition is met, then stops
- `compress(vals, selector)` - keeps `vals` based on true/false specified in `selector`

### Infinite Iterators
- `count(start, step)` - infinitely counts from `start` by `step`
- `cycle(iterator)` - infinitely cycles through the vals in `iterator`
- `repeat(val)` - infinitely repeats `val`, yielding the object at the same memory address each time

### Chaining and Teeing
- `chain(*iterables)` - concatenate multiple iterables together
- `tee(iterable, n)` - create `n` *shallow* copies of `iterable`. If `iterable` contains iterables, these will be exhausted!

### Mapping and Reducing
- `map(fn, iterable)` - apply a function to all values in an iterable
- `starmap(fn, iterable)` - apply a function to all sub-iterables within an iterable
- `reduce(fn, iterable)` - turn an iterable into a single value. Default behavior is addition. Use the `operator` module to specify other operations (ex. `operator.mul` for multiplication)
- `accumulate(iterable, fn)` - yield intermediate values generated by the `reduce` fn. Combine with the `chain` fn to emulate a start condition (ex. multiply all values by 10 then add them together)

### Zipping
- `zip` - combine 2+ iterables to iterate in parallel. The shortest iterable dictates the amount of values that can be yielded
- `zip_longest(*iterables, fillvalue)` - allow the longest iterable to determine the amount of yielded values, specifying a `fillvalue` for iterables that run out of values early

### Grouping
- `groupby(iterable, key)` - group data and return a list of tuples - (group, iterator) for unique `key` vals in `iterable`. The default `key` is the data itself

### Combinatorics
- `product(*iterables)` - returns the cartesian product of the iterables as a list of tuples. Position matters for cartesian products, so `(2,1)` != `(1,2)` and they both will be returned
- `permutations(iterable, n)` - returns all permutations of length `n` from `iterable`. Again, position matters
- `combinations(iterable, n)` - returns all combinations of length `n` from `iterable`. Position does not matter, so `(2,1)` == `(1,2)`, and only the first occurence will be returned
- `combinations_with_replacement(iterable, n)` - returns all combinations of length `n` from `iterable` *with replacement*. In this way, `(1,1)` will be returned, when it would not be returned from `combinations`

## Context Managers

- See [PEP 343](https://peps.python.org/pep-0343/)

### Intro
- Context managers do 2 things
    - Open something with the `__enter__` method
    - Close something when you leave with the `__exit__` method
- Potential use cases include:
    - Opening/writing to files
    - Locking/releasing resources (threading)
    - Changing/resetting
        - ex. Change precision then reset once done
    - Redirecting stdout to a file
    - Enter/exit
        - ex. DB transaction -> upon exit, either commit or rollback the transaction

### Context Managers and Iterators
- Implement both the context manager protocol and the iterator protocol to create an iterator that can also behave like a context manager

### Additional Uses
- `getcontext()` returns the Context of an object
    - ex. `decimal.getcontext() = Context(prec=28, rounding=...)`
- Use `sys.stdout = open(fname, 'w')` to redirect print statements to fname
- Context managers can be nested to build things like nested lists

### Generators and Context Managers
- Use a `try ... yield/finally` block to emulate a context manager with a generator
    - Decorate the generator with `@contextmanager` from `contextlib` to let python implement the context manager protocol
- Use `ExitStack` from `contextlib` to nest multiple contexts
    - ex. You want to iterate through several files and don't want to type `with open(fname, 'r') as f` for each file
